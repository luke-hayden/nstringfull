---
title: "Putting the CART before the horse"
author: "Luke Hayden"
date: 26th September 2018
output:
  beamer_presentation:
    theme: "Darmstadt"
    colortheme: "fly"
    fonttheme: "default"
slide_level: 3
---



```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
library(dplyr)
library(ggbiplot)
library(tibble)
library(tidyr)
library(caret)
library(RColorBrewer)
library(ggrepel)
library(gtools)
library(FinCal)
library(ggrepel)
library(reshape2)
#library(e10)
````



```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#setwd("~/Documents/nstringjul18/nstringfull")
load(file="allns_data.rdata")
load(file="modelrf.rdata")
load(file="modelgbm.rdata")
load(file="hkdat.rdata")
load(file="sampleinfo.rdata")

load(file="goodgroups.rdata")


````




#Multiple Regression approach
####Simple linear regression: 

**Age = X(marker1) + c**

We try to find values for x & c that come as close as possible to solving the equation for each set of values for *Age* and *marker1* we have. 

####Two predictors:

**Age = X(marker1) + Y(marker2) + c**

####Many predictors

**Age = X(marker1) + Y(marker2) + Z(marker3) + W(marker4) + .... + c**


Where we have many different markers, we can find values of x,y,z,w, etc that solve this equation very well but don't provide predictive power: we call this overfitting


#How do we avoid overfitting?

##We want: 




#Random Forest approach

###Decision tree

Classify or perform regression by asking binary questions of data: whether value of marker X is above or below key value Y, whther marker Z is above or below.....

###Random Forest

Ensemble of decision trees, each using a random subset of the predictors

Resists overfitting






#Background
##Project

Examine the effect of regeneration on the molecular age profile of *Parhyale* limbs

##Designing codeset

*Nanostring as method to quantify gene expression

*200 genes in codeset

-195 genes chosen on the basis of differential expression analysis 

-5 control genes: do not vary in expression between conditions



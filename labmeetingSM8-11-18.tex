\documentclass[ignorenonframetext,]{beamer}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme[]{Darmstadt}
\usecolortheme{fly}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\newif\ifbibliography
\hypersetup{
            pdftitle={Putting the CART before the horse},
            pdfauthor={Luke Hayden},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls

% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom

\AtBeginPart{
  \let\insertpartnumber\relax
  \let\partname\relax
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \let\insertsectionnumber\relax
    \let\sectionname\relax
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \let\insertsubsectionnumber\relax
  \let\subsectionname\relax
  \frame{\subsectionpage}
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

\title{Putting the CART before the horse}
\author{Luke Hayden}
\date{26th September 2018}

\begin{document}
\frame{\titlepage}

\begin{frame}{Classic modelling}

\begin{block}{Multiple Regression approach}

\begin{block}{Simple linear regression:}

\textbf{Age = X(marker1) + c}

We try to find values for x \& c that come as close as possible to
solving the equation for each set of values for \emph{Age} and
\emph{marker1} we have.

\end{block}

\begin{block}{Two predictors:}

\textbf{Age = X(marker1) + Y(marker2) + c}

\end{block}

\begin{block}{Many predictors}

\textbf{Age = X(marker1) + Y(marker2) + Z(marker3) + W(marker4) +
\ldots{}. + c}

Where we have many different markers, we can find values of x,y,z,w, etc
that solve this equation very well but don't provide predictive power:
we call this overfitting

\end{block}

\end{block}

\end{frame}

\begin{frame}{How do we avoid overfitting?}

\begin{block}{We want:}

Modelling approach that can capture the signal without simply
reproducing all the noise present in our dataset

To maximise predictive power

\end{block}

\begin{block}{Approaches:}

\begin{block}{Data partitioning:}

train-test split

cross-validation)

\end{block}

\begin{block}{Model type}

Ensemble methods!

\end{block}

\begin{block}{Model parameters}

Exploring parameter space

\end{block}

\end{block}

\end{frame}

\begin{frame}{Machine Learning terminology}

\begin{block}{Supervised vs unsupervised learning}

Unsupervised learning: find the shape of the data (

(eg: PCA, kmeans clustering)

Supervised learning: train an algorithm to recapitulate the examples it
sees in a dataset

(eg: linear regression)

\end{block}

\begin{block}{Classification vs Regression}

Classification: categorise examples into one of a number of discrete
categories

Regression: determine value along range

\end{block}

\end{frame}

\begin{frame}{Tree ensemble approaches}

\begin{block}{Decision tree}

Classify or perform regression by asking binary questions of data:
whether value of marker X is above or below key value Y, whther marker Z
is above or below\ldots{}..

\end{block}

\begin{block}{Random Forest}

Ensemble of decision trees, each using a random subset of the predictors
to classify/perform regression on a random subset of the data

Resists overfitting

\end{block}

\begin{block}{Gradient Boosting Machine}

Start with simple model (eg: mean of values in training dataset)

\end{block}

\end{frame}

\begin{frame}{Random Forest parameters}

ntree: number of trees

mtry: Number of variables randomly sampled as candidates at each split

min.node.size: sets depth of trees

cross-validation folds: number of repartitions of data for testing

splitting model: variance or ``extratrees''

\end{frame}

\begin{frame}{My project as example}

\begin{block}{Project}

Examine the effect of regeneration on the molecular age profile of
\emph{Parhyale} limbs

\end{block}

\begin{block}{Designing codeset}

*Nanostring as method to quantify gene expression

*200 genes in codeset

-195 genes chosen on the basis of differential expression analysis

-5 control genes: do not vary in expression between conditions

\end{block}

\end{frame}

\end{document}

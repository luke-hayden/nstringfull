---
title: "Man vs model: random forest tuning and optimisation"
author: "Luke Hayden"
date: "Oct 1st, 2018"
output: html_document
---



```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}

#Set-up and data import
library(dplyr)
library(ggbiplot)
library(tibble)
library(tidyr)
library(caret)
library(RColorBrewer)
library(ggrepel)
library(gtools)
library(FinCal)
````




```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#setwd("~/Documents/nstringjul18")
load(file="allns_data.rdata")


#save(cvdf,lmdf, lmdf2,  file="vardat.rdata")
load(file="vardat.rdata")
load(file="models.rdata")

````
#Optimising random forest

I have been working on fine-tuning the parameters of the random forest in order to improve its predictive power. In particular, I have been focused on dealing with the overfitting issue. I have been playing around with the following parameters:

ntree: number of trees

mtry: Number of variables randomly sampled as candidates at each split

min.node.size: sets depth of trees

cross-validation folds: number of repartitions of data for testing 

splitting model: variance or "extratrees"

This is how they affect the model, below. Note: lower RMSE is better. 





```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#Random forest
md <- as.data.frame(subset(t(ctall.norm), sampleinfo$sex == "F" &

                 sampleinfo$prep== "Luke"&
                 sampleinfo$exp %in% c("Size-age", "cohorts", "OvY") &
                 sampleinfo$ctg>2 &
                sampleinfo$codeset == "phaw_1" &
                 sampleinfo$type %in% c("O", "Y", "M")
&   !(sampleinfo$sample %in% c("T","C", "U", "LFPD", "O1a", "SFPA", "LFIB","LFIA","AF", "Coh8"))
))
md <- md[, lmdf2$in61 ==T & lmdf2$pvalovy < 0.05]

#md <- md[,colnames(md) %in% currbest$coefnames]





md$sample <- rownames(md)

md <- left_join(md, select(sampleinfo, sample, predage), by="sample") %>%
  filter(!(is.na(predage))) %>%
  column_to_rownames(var="sample")


trainchoice <- sample(1:nrow(md))[1:floor(4*(nrow(md)/5))]


trainchoice <- rownames(md) %in% rownames(currbest$trainingData)
trdat <- md[trainchoice,]
tedat <- md[-trainchoice,]



rfagemodel <- train(
  predage~.,
  tuneLength = 4,
  metric="RMSE",
  num.trees=2000,
  importance = "permutation",
  data = trdat, method = "ranger",
  tuneGrid=expand.grid(mtry=c(10:20),
                        splitrule=c("extratrees", "variance"),
                        min.node.size=c(1:10)),
  trControl = trainControl(method = "cv", number = 1, verboseIter = T)
)
```




```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
plot(currbest)
#rfagemodel$trainingData
#summary(currbest$finalModel)
````


The last of these has had by far the most importance in dealing with overfitting. Using most (47) of the 61 markers from the 2nd codeset, we get the followign model.



##Model accuracy: age from body length regression

The fit is like this:

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
mymodel <- currbest
#mymodel <- rfagemodel

md <- as.data.frame(subset(t(ctall.norm), sampleinfo$sex == "F" &
#               sampleinfo$qual == "ok"& 
                 sampleinfo$prep== "Luke"&
                 sampleinfo$exp %in% c("Size-age", "cohorts", "OvY") & 
 #                sampleinfo$ctg>2 & 
                sampleinfo$codeset == "phaw_1" &
                 sampleinfo$type %in% c("O", "Y", "M")& 
  !(sampleinfo$sample %in% c("T","C", "U", "LFPD", "O1a", "SFPA", "LFIB","LFIA","AF", "Coh8"))))



md <- md[,colnames(md) %in% mymodel$coefnames]



md$rfagepred <- predict(mymodel, newdata=md)


md$intrain <- rownames(md) %in% rownames(mymodel$trainingData)



md <- md %>%
  rownames_to_column(var="sample") %>%
  left_join(sampleinfo,  by="sample")

md <- subset(md, !(is.na(md$predage)))


RMSEtr <- sqrt(mean( (md$rfagepred[md$intrain == T] - md$predage[md$intrain == T])   ^2))





RMSEte <- sqrt(mean( (md$rfagepred[md$intrain == F] - md$predage[md$intrain == F])   ^2))

(p=ggplot(md, aes(x=predage, y=rfagepred,colour=intrain))+
  geom_point()+
  xlab("Age (weeks) from body length") +
  ylab("Age (weeks) from marker gene expression (random forest model)")+
  geom_smooth(method="lm", colour="black")+
  theme_bw()+
 scale_colour_manual(values=c("cornflower blue", "red3"), name="Data partition", labels=c("Test", "Training"))+
  geom_text_repel(aes(label=sample), size=2.5)+
  ggtitle("Using Marker gene expression to predict age", 
          subtitle=paste0("RMSE in training data: ", round(RMSEtr,3), 
                          "\nRMSE in test data: ", round(RMSEte,3))))#+  facet_wrap(~intrain)


ggsave(plot=p,height=5,width=6,dpi=200, filename=paste("modeltrte.pdf"), useDingbats=FALSE, limitsize = FALSE)


```

Which looks to be a pretty major improvement in terms of dealing with the overfitting. Some additional optimisation may help some more. 


```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
##Model accuracy: cohort age data
#   
# (p=ggplot(md, aes(x=wkage, y=rfagepred))+
#   geom_point()+
#   xlab("Age (weeks): cohorts") +
#   ylab("Age (weeks) from marker gene expression (random forest model)")+
#   geom_smooth(method="lm")+
#   theme_bw()+
# # scale_colour_manual(values=c("darkviolet", "cornflower blue", "red3"))+
#   geom_text_repel(aes(label=sample), size=2.5)+
#   ggtitle("Using Marker gene expression to predict age", subtitle=paste("R^2 of correlation between actual age and \npredicted age based on gene expression with random forest model:", round(summary(lm(as.numeric(wkage)~rfagepred, md))$r.squared, 3)))
# )


```


```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#save(currbest,file="models.rdata")

```







```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
###Random forest: current best attempt
# md <- as.data.frame(subset(t(ctall.norm), sampleinfo$sex == "F" &
# #               sampleinfo$qual == "ok"&
#                  sampleinfo$prep== "Luke"&
#                  sampleinfo$exp %in% c("Size-age", "cohorts", "OvY") &
#  #                sampleinfo$ctg>2 &
#                 sampleinfo$codeset == "phaw_1" &
#                  sampleinfo$type %in% c("O", "Y", "M")
# &   !(sampleinfo$sample %in% c("T","C", "U", "LFPD", "O1a", "SFPA", "LFIB","LFIA","AF", "Coh8"))
# ))
# 
# #md <- md[,minf$chosenand ==T]
# md <- md[, lmdf2$in61 ==T & lmdf2$pvalovy < 0.03]
# 
# 
# md$sample <- rownames(md)
# 
# md <- left_join(md, select(sampleinfo, sample, predage), by="sample") %>%
#   filter(!(is.na(predage))) %>%
#   column_to_rownames(var="sample")
# 
# 
# trainchoice <- sample(1:nrow(md))[1:floor(4*(nrow(md)/5))]
# 
# 
# #trainchoice <- rownames(md) %in% rownames(currbest$trainingData)
# trdat <- md[trainchoice,]
# tedat <- md[-trainchoice,]
# 
# 
# 
# rfagemodel <- train(
#   predage~.,
#   tuneLength = 4,
#   metric="RMSE",
#   data = trdat, method = "ranger",
#   tuneGrid=expand.grid(mtry=c(10:20),
#                         splitrule=c("extratrees", "variance"),
#                         min.node.size=c(1:10)),
#   trControl = trainControl(method = "cv", number = 40, verboseIter = T)
# )
```





#Interrogating the model


Markers used:


```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}

mymodel <- currbest
#mymodel <- rfagemodel

mymodel$coefnames


````

####Variable importance

Variable importance obtained via permutation 

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}


vrimp <-varImp(mymodel)$importance %>%
  rownames_to_column(var="name")
vrimp$sname <- substr(vrimp$name, 1,2)

vrimp$sname <- factor(vrimp$sname, levels=vrimp$sname[order(vrimp$Overall, decreasing=T)])

(p=ggplot(vrimp, aes(x=sname, y=Overall))+
  theme_bw()+
  geom_bar(stat="identity", fill="red3")+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))+
  xlab("Marker")+
  ylab("Contribution to model")+
  ggtitle("Marker contributions", subtitle= paste0("random forest model built with ", length(mymodel$coefnames), " markers"))
)

ggsave(plot=p,height=5,width=5,dpi=200, filename=paste("contribs.pdf"), useDingbats=FALSE, limitsize = FALSE)
````


One marker is a huge outlier in this respect, contributing more than twice as much as the next most important marker. 

##Marker profiles

I have also graphed the expression of each marker over time. 


```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, fig.height = 20}


md <- as.data.frame(subset(t(ctall.norm), sampleinfo$sex == "F" &
#               sampleinfo$qual == "ok"& 
                 sampleinfo$prep== "Luke"&
                 sampleinfo$exp %in% c("Size-age", "cohorts", "OvY") & 
 #                sampleinfo$ctg>2 & 
                sampleinfo$codeset == "phaw_1" &
                 sampleinfo$type %in% c("O", "Y", "M")& 
  !(sampleinfo$sample %in% c("T","C", "U", "LFPD", "O1a", "SFPA", "LFIB","LFIA","AF", "Coh8"))))



md <- md[,colnames(md) %in% mymodel$coefnames]
md <- subset(md, rownames(md) %in% sampleinfo$sample[!is.na(sampleinfo$predage)])

md <-  as.data.frame(md/rowMeans(md))

md <- md %>%
  rownames_to_column(var="sample") %>%
  left_join(sampleinfo,  by="sample") %>%
  gather(key=marker, value= exp, mymodel$coefnames)%>%
  left_join(minf, by=c("marker"="name"))%>%
  left_join(select(vrimp, -sname), by= c("marker"="name"))

md$lab <- paste0(md$sname, ": ", round(md$Overall,3))


md$lab <- factor(md$lab, levels=unique(md$lab[order(md$Overall, decreasing=T)]))

(p=ggplot(md, aes(x=predage, y= exp, colour=dir))+
  geom_smooth()+
  geom_point(size=0.5)+
  theme_bw()+
  facet_wrap(~lab, scales="free", ncol=3)+
  scale_colour_brewer(palette="Set1")+
  xlab("Age (based on body length)")+
  ylab("Normalised expression")+
  theme(legend.direction = 'horizontal', legend.position = 'bottom')+
 
  ggtitle("Marker profiles", subtitle= paste0("markers from random forest model built with ", length(mymodel$coefnames), " markers")) )



ggsave(plot=p,height=18,width=6,dpi=200, filename=paste("contribprofiles.pdf"), useDingbats=FALSE, limitsize = FALSE)



````

It looks like some optimisation on the basis of these profiles may furhter improve the model's fit. 
